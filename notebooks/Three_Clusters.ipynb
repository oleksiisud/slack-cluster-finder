{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7b670c",
   "metadata": {},
   "source": [
    "HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer  # using HuggingFace model\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def cluster_and_query(text, query=None):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < 2:\n",
    "        return \"Please enter at least two sentences.\", None, None\n",
    "    \n",
    "    # Embed sentences\n",
    "    embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "    \n",
    "    # Reduce to 2D for visualization\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.3, metric='cosine', random_state=42)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Cluster\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')\n",
    "    labels = clusterer.fit_predict(reduced)\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(7,5))\n",
    "    scatter = plt.scatter(reduced[:,0], reduced[:,1], c=labels, cmap='tab10', s=60)\n",
    "    plt.title(\"Sentence Clusters\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    \n",
    "    # Annotate sentences\n",
    "    for i, s in enumerate(sentences):\n",
    "        plt.annotate(str(i), (reduced[i,0], reduced[i,1]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cluster_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Group sentences by cluster\n",
    "    grouped = {}\n",
    "    for sent, lbl in zip(sentences, labels):\n",
    "        grouped.setdefault(int(lbl), []).append(sent)\n",
    "    \n",
    "    summary = \"\\n\\n\".join([f\"Cluster {c}:\\n\" + \"\\n\".join(v) for c, v in grouped.items()])\n",
    "    \n",
    "    # Query functionality\n",
    "    query_result = \"\"\n",
    "    if query:\n",
    "        query_emb = model.encode([query], normalize_embeddings=True)\n",
    "        sims = np.dot(embeddings, query_emb.T).flatten()  # cosine similarity\n",
    "        top_idx = np.argsort(-sims)[:5]  # top 5 matches\n",
    "        query_result = \"\\n\".join([f\"{i+1}. {sentences[idx]}\" for i, idx in enumerate(top_idx)])\n",
    "    \n",
    "    return summary, \"cluster_plot.png\", query_result\n",
    "\n",
    "# Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=cluster_and_query,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=10, placeholder=\"Paste article or chat transcript here...\"),\n",
    "        gr.Textbox(lines=1, placeholder=\"Enter query to search relevant sentences (optional)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Clustered Sentences\"),\n",
    "        gr.Image(label=\"Cluster Graph\"),\n",
    "        gr.Textbox(label=\"Top Sentences for Query\")\n",
    "    ],\n",
    "    title=\"Contextual Sentence Clustering with Query\",\n",
    "    description=\"Enter an article or chat log. The model embeds each sentence, clusters by meaning, shows a 2D cluster map, and optionally finds sentences matching your query.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5e887",
   "metadata": {},
   "source": [
    "KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2411ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# --- Safe NLTK setup (top-level, before threads) ---\n",
    "nltk_data_dir = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# Append the directory first\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Download punkt_tab if missing\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab', download_dir=nltk_data_dir)\n",
    "\n",
    "# Import tokenizer AFTER setup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# # --- Safe NLTK setup ---\n",
    "# nltk_data_dir = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "# os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except LookupError:\n",
    "#     nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "\n",
    "# nltk.data.path.append(nltk_data_dir)\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# --- Load embedding model ---\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def cluster_article(text, query=None, n_clusters=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < 2:\n",
    "        return \"Please enter at least two sentences.\", None, None\n",
    "\n",
    "    # Embed sentences\n",
    "    embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "\n",
    "    # Reduce dimensions for visualization\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.3, metric='cosine', random_state=42)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Cluster with KMeans\n",
    "    kmeans = KMeans(n_clusters=min(n_clusters, len(sentences)), random_state=42)\n",
    "    labels = kmeans.fit_predict(reduced)\n",
    "\n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(7,5))\n",
    "    scatter = plt.scatter(reduced[:,0], reduced[:,1], c=labels, cmap='tab10', s=60)\n",
    "    plt.title(\"Sentence Clusters\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "\n",
    "    # Annotate sentences\n",
    "    for i, s in enumerate(sentences):\n",
    "        plt.annotate(str(i), (reduced[i,0], reduced[i,1]))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cluster_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Group sentences by cluster\n",
    "    grouped = {}\n",
    "    for sent, lbl in zip(sentences, labels):\n",
    "        grouped.setdefault(int(lbl), []).append(sent)\n",
    "    summary = \"\\n\\n\".join([f\"Cluster {c}:\\n\" + \"\\n\".join(v) for c, v in grouped.items()])\n",
    "\n",
    "    # Find most similar sentences if query is provided\n",
    "    if query:\n",
    "        query_emb = model.encode([query], normalize_embeddings=True)\n",
    "        sims = cosine_similarity(query_emb, embeddings)[0]\n",
    "        top_idx = np.argsort(sims)[::-1][:5]  # Top 5 matches\n",
    "        similar_sentences = [sentences[i] for i in top_idx]\n",
    "        similar_summary = \"\\n\".join(similar_sentences)\n",
    "    else:\n",
    "        similar_summary = \"\"\n",
    "\n",
    "    return summary, \"cluster_plot.png\", similar_summary\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "demo = gr.Interface(\n",
    "    fn=cluster_article,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=10, placeholder=\"Paste article or chat transcript here...\"),\n",
    "        gr.Textbox(lines=1, placeholder=\"Optional: Enter search query here...\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Clustered Sentences\"),\n",
    "        gr.Image(label=\"Cluster Graph\"),\n",
    "        gr.Textbox(label=\"Most Similar Sentences (Optional)\")\n",
    "    ],\n",
    "    title=\"Contextual Sentence Clustering\",\n",
    "    description=\"Enter an article or chat log. The model embeds each sentence, clusters by meaning, shows a 2D cluster map, and optionally finds sentences most similar to your query.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def cluster_with_clarans(text, query=None, k=3, numlocal=2, maxneighbor=10):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < 2:\n",
    "        return \"Please enter at least two sentences.\", None, None\n",
    "    \n",
    "    # Embed sentences\n",
    "    embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "    \n",
    "    # Reduce to 2D for visualization\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.3, metric='cosine', random_state=42)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Distance matrix for CLARANS (use reduced dimensionality for speed)\n",
    "    distance_matrix = euclidean_distances(reduced).tolist()\n",
    "    \n",
    "    # Run CLARANS with reduced search parameters for speed\n",
    "    clarans_instance = clarans(distance_matrix, number_clusters=k, numlocal=numlocal, maxneighbor=maxneighbor)\n",
    "    clarans_instance.process()\n",
    "    clusters = clarans_instance.get_clusters()\n",
    "    \n",
    "    # Map sentences to cluster labels\n",
    "    labels = np.full(len(sentences), -1)\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for idx in cluster:\n",
    "            labels[idx] = cluster_id\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(7,5))\n",
    "    scatter = plt.scatter(reduced[:,0], reduced[:,1], c=labels, cmap='tab10', s=60)\n",
    "    plt.title(f\"Sentence Clusters (CLARANS k={k})\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    \n",
    "    for i, s in enumerate(sentences):\n",
    "        plt.annotate(str(i), (reduced[i,0], reduced[i,1]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cluster_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Group sentences by cluster\n",
    "    grouped = {}\n",
    "    for sent, lbl in zip(sentences, labels):\n",
    "        grouped.setdefault(int(lbl), []).append(sent)\n",
    "    \n",
    "    summary = \"\\n\\n\".join([f\"Cluster {c}:\\n\" + \"\\n\".join(v) for c, v in grouped.items()])\n",
    "    \n",
    "    # Query functionality\n",
    "    query_result = \"\"\n",
    "    if query:\n",
    "        query_emb = model.encode([query], normalize_embeddings=True)\n",
    "        sims = np.dot(embeddings, query_emb.T).flatten()  # cosine similarity\n",
    "        top_idx = np.argsort(-sims)[:5]  # top 5 matches\n",
    "        query_result = \"\\n\".join([f\"{i+1}. {sentences[idx]}\" for i, idx in enumerate(top_idx)])\n",
    "    \n",
    "    return summary, \"cluster_plot.png\", query_result\n",
    "\n",
    "# Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=cluster_with_clarans,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=10, placeholder=\"Paste article or chat transcript here...\"),\n",
    "        gr.Textbox(lines=1, placeholder=\"Enter query to search relevant sentences (optional)\"),\n",
    "        gr.Slider(minimum=2, maximum=10, step=1, value=3, label=\"Number of Clusters (k)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Clustered Sentences\"),\n",
    "        gr.Image(label=\"Cluster Graph\"),\n",
    "        gr.Textbox(label=\"Top Sentences for Query\")\n",
    "    ],\n",
    "    title=\"CLARANS Sentence Clustering with Query\",\n",
    "    description=\"Clusters sentences using CLARANS (k-medoids variant), shows 2D map, and returns top sentences matching query. Note: CLARANS is slower than other algorithms.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c486bc7",
   "metadata": {},
   "source": [
    "Clarnas Attempted Speed Improvements\n",
    "\n",
    "Use reduced 2D data instead of full embeddings - Calculates distances on the 2D UMAP projection instead of the full 384-dimensional embeddings, dramatically reducing computation\n",
    "Reduced numlocal from 5 to 2 - Fewer random restarts\n",
    "Reduced maxneighbor from 20 to 10 - Less exhaustive neighborhood search\n",
    "Use sklearn's euclidean_distances instead of pyclustering's calculate_distance_matrix - More optimized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0dc0ae",
   "metadata": {},
   "source": [
    "Combined Comparison: All Three Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab281554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyclustering.cluster.clarans import clarans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def run_all_clustering(text, query=None):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < 2:\n",
    "        return \"Please enter at least two sentences.\", None, None, None, None, None, \"\", \"\", \"\"\n",
    "    \n",
    "    # Embed sentences\n",
    "    embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "    \n",
    "    # Reduce to 2D for visualization\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.3, metric='cosine', random_state=42)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # --- HDBSCAN Clustering ---\n",
    "    start_time = time.time()\n",
    "    hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=2, metric='euclidean')\n",
    "    hdbscan_labels = hdbscan_clusterer.fit_predict(reduced)\n",
    "    hdbscan_time = time.time() - start_time\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(reduced[:,0], reduced[:,1], c=hdbscan_labels, cmap='tab10', s=60)\n",
    "    plt.title(\"HDBSCAN Clustering\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    for i in range(len(sentences)):\n",
    "        plt.annotate(str(i), (reduced[i,0], reduced[i,1]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"hdbscan_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    hdbscan_grouped = {}\n",
    "    for sent, lbl in zip(sentences, hdbscan_labels):\n",
    "        hdbscan_grouped.setdefault(int(lbl), []).append(sent)\n",
    "    hdbscan_summary = f\"⏱️ Time: {hdbscan_time:.2f}s\\n\\n\" + \"\\n\\n\".join([f\"Cluster {c}:\\n\" + \"\\n\".join(v) for c, v in hdbscan_grouped.items()])\n",
    "    \n",
    "    # --- KMeans Clustering ---\n",
    "    start_time = time.time()\n",
    "    k = min(5, len(sentences))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(reduced)\n",
    "    kmeans_time = time.time() - start_time\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(reduced[:,0], reduced[:,1], c=kmeans_labels, cmap='tab10', s=60)\n",
    "    plt.title(f\"KMeans Clustering (k={k})\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    for i in range(len(sentences)):\n",
    "        plt.annotate(str(i), (reduced[i,0], reduced[i,1]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"kmeans_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    kmeans_grouped = {}\n",
    "    for sent, lbl in zip(sentences, kmeans_labels):\n",
    "        kmeans_grouped.setdefault(int(lbl), []).append(sent)\n",
    "    kmeans_summary = f\"⏱️ Time: {kmeans_time:.2f}s\\n\\n\" + \"\\n\\n\".join([f\"Cluster {c}:\\n\" + \"\\n\".join(v) for c, v in kmeans_grouped.items()])\n",
    "    \n",
    "    # --- CLARANS Clustering ---\n",
    "    start_time = time.time()\n",
    "    k_clarans = min(3, len(sentences))\n",
    "    distance_matrix = euclidean_distances(reduced).tolist()\n",
    "    clarans_instance = clarans(distance_matrix, number_clusters=k_clarans, numlocal=2, maxneighbor=10)\n",
    "    clarans_instance.process()\n",
    "    clusters = clarans_instance.get_clusters()\n",
    "    clarans_time = time.time() - start_time\n",
    "    \n",
    "    clarans_labels = np.full(len(sentences), -1)\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for idx in cluster:\n",
    "            clarans_labels[idx] = cluster_id\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(reduced[:,0], reduced[:,1], c=clarans_labels, cmap='tab10', s=60)\n",
    "    plt.title(f\"CLARANS Clustering (k={k_clarans})\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    for i in range(len(sentences)):\n",
    "        plt.annotate(str(i), (reduced[i,0], reduced[i,1]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"clarans_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    clarans_grouped = {}\n",
    "    for sent, lbl in zip(sentences, clarans_labels):\n",
    "        clarans_grouped.setdefault(int(lbl), []).append(sent)\n",
    "    clarans_summary = f\"⏱️ Time: {clarans_time:.2f}s\\n\\n\" + \"\\n\\n\".join([f\"Cluster {c}:\\n\" + \"\\n\".join(v) for c, v in clarans_grouped.items()])\n",
    "    \n",
    "    # --- Query Results (separate for each algorithm to show differences) ---\n",
    "    hdbscan_query = kmeans_query = clarans_query = \"\"\n",
    "    if query:\n",
    "        query_emb = model.encode([query], normalize_embeddings=True)\n",
    "        sims = np.dot(embeddings, query_emb.T).flatten()\n",
    "        top_idx = np.argsort(-sims)[:5]\n",
    "        \n",
    "        # Base query results\n",
    "        base_results = [f\"{i+1}. {sentences[idx]} (similarity: {sims[idx]:.3f})\" for i, idx in enumerate(top_idx)]\n",
    "        \n",
    "        # Add cluster info for each algorithm\n",
    "        hdbscan_query = \"Top 5 sentences matching query:\\n\\n\" + \"\\n\\n\".join(\n",
    "            [f\"{base_results[i]} [HDBSCAN Cluster: {hdbscan_labels[top_idx[i]]}]\" for i in range(len(base_results))]\n",
    "        )\n",
    "        \n",
    "        kmeans_query = \"Top 5 sentences matching query:\\n\\n\" + \"\\n\\n\".join(\n",
    "            [f\"{base_results[i]} [KMeans Cluster: {kmeans_labels[top_idx[i]]}]\" for i in range(len(base_results))]\n",
    "        )\n",
    "        \n",
    "        clarans_query = \"Top 5 sentences matching query:\\n\\n\" + \"\\n\\n\".join(\n",
    "            [f\"{base_results[i]} [CLARANS Cluster: {clarans_labels[top_idx[i]]}]\" for i in range(len(base_results))]\n",
    "        )\n",
    "    \n",
    "    return (hdbscan_summary, \"hdbscan_plot.png\", hdbscan_query,\n",
    "            kmeans_summary, \"kmeans_plot.png\", kmeans_query,\n",
    "            clarans_summary, \"clarans_plot.png\", clarans_query)\n",
    "\n",
    "# Gradio interface with all three algorithms\n",
    "demo = gr.Interface(\n",
    "    fn=run_all_clustering,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=10, placeholder=\"Paste article or chat transcript here...\", label=\"Text Input\"),\n",
    "        gr.Textbox(lines=1, placeholder=\"Enter query to search relevant sentences (optional)\", label=\"Query (Optional)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"HDBSCAN: Clustered Sentences (with timing)\"),\n",
    "        gr.Image(label=\"HDBSCAN: Cluster Graph\"),\n",
    "        gr.Textbox(label=\"HDBSCAN: Query Results\"),\n",
    "        gr.Textbox(label=\"KMeans: Clustered Sentences (with timing)\"),\n",
    "        gr.Image(label=\"KMeans: Cluster Graph\"),\n",
    "        gr.Textbox(label=\"KMeans: Query Results\"),\n",
    "        gr.Textbox(label=\"CLARANS: Clustered Sentences (with timing)\"),\n",
    "        gr.Image(label=\"CLARANS: Cluster Graph\"),\n",
    "        gr.Textbox(label=\"CLARANS: Query Results\")\n",
    "    ],\n",
    "    title=\"Clustering Comparison: HDBSCAN vs KMeans vs CLARANS\",\n",
    "    description=\"Compare three clustering algorithms side-by-side. Enter text to cluster sentences and optionally query for relevant sentences. Timing and cluster assignments are shown for each algorithm.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa4fb5",
   "metadata": {},
   "source": [
    "Example article: \n",
    "https://en.wikipedia.org/wiki/Tokyo\n",
    "\n",
    "Example Queries\n",
    "Big Island\n",
    "Tasty Food\n",
    "Economic Growth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
